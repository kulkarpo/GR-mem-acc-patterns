{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install influxdb\n",
    "!pip install pandas\n",
    "!pip install numpy\n",
    "!pip install scikit-learn\n",
    "!pip install ruptures\n",
    "!pip install PrettyTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from influxdb import InfluxDBClient\n",
    "import pandas as pd\n",
    "\n",
    "# required for learning\n",
    "import numpy as np\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.utils.validation import check_array\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from math import sqrt\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.linear_model import TheilSenRegressor\n",
    "from scipy import stats\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# required for printing results\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "# required only for ploting\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize']= 15,6\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Host 1\n",
    "host1 = \"138.246.232.174\"\n",
    "#Host 2\n",
    "host2 = \"138.246.232.219\"\n",
    "\n",
    "port = 8086\n",
    "user = \"root\"\n",
    "password = \"root\"\n",
    "#dbname = \"nodejs-gc-test-leak\" #todo parameterize this\n",
    "\n",
    "#!mkdir ./plots/$dbname\n",
    "\n",
    "imagedict = { \"138.246.232.174\": {\n",
    "    'nodejs-gc-test-leak': {\n",
    "        'image' : 'poojakulkarni/nodegctestleak:latest',\n",
    "        'starttime': '2019-07-20T03:42:21.417Z',\n",
    "        'endtime': '2019-07-20T06:43:31.417Z'\n",
    "    }, \n",
    "    'script-basic-java-sleep1min-leak':{ # actually 3min sleep\n",
    "        'image': 'poojakulkarni/javabasicleak3minsleep:latest',\n",
    "        'starttime': '2019-07-19T12:25:23.172Z',\n",
    "        'endtime': '2019-07-19T15:26:38.172Z'\n",
    "    },\n",
    "    'latest-basic-java-sleep1min-leak':{ # 1min sleep\n",
    "        'image': 'poojakulkarni/javabasicleak1minsleep:latest',\n",
    "        'starttime': '2019-07-25T05:18:00.64Z',\n",
    "        'endtime': '2019-07-25T08:18:10.64Z'\n",
    "    },\n",
    "    'script-mem-alloc-cffi-leak-try3':{\n",
    "        'image': 'poojakulkarni/memallocpyleak:latest',\n",
    "        'starttime': '2019-07-20T00:38:13.62Z',\n",
    "        'endtime': '2019-07-20T03:39:28.621Z'\n",
    "     },\n",
    "    'noodejs-event-emitter-leak':{\n",
    "        'image':'poojakulkarni/nodejseventemitterleak:latest',\n",
    "        'starttime': '2019-07-22T05:11:39.471Z',\n",
    "        'endtime': '2019-07-22T08:11:49.471Z'\n",
    "    },\n",
    "    'script-basic-python-sleep1min-verified-leak':{\n",
    "        'image': 'poojakulkarni/pythonbasicleak1minsleep:latest',\n",
    "        'starttime': '2019-07-19T18:31:55.291Z',\n",
    "        'endtime': '2019-07-19T21:33:05.291Z'\n",
    "    },\n",
    "    'nodejs-basic-leak': {\n",
    "        'image' : 'poojakulkarni/nodejsbasicleak:latest',\n",
    "        'starttime': '2019-07-22T18:11:01.579Z',\n",
    "        'endtime': '2019-07-22T21:11:46.579Z'\n",
    "    },\n",
    "    'script-java-hashmap-leak': {\n",
    "        'image' : 'poojakulkarni/javahashmapleak:latest',\n",
    "        'starttime': '2019-08-06T05:33:16.589Z',\n",
    "        'endtime': '2019-08-06T08:34:26.589Z'\n",
    "    },\n",
    "    'script-java-inflater-g1-leak': {\n",
    "        'image' : 'poojakulkarni/javainflaterg1leak:latest',\n",
    "        'starttime': '2019-08-08T04:15:31.024Z',\n",
    "        'endtime': '2019-08-08T07:16:41.024Z'\n",
    "    },\n",
    "    'script-java-inflater-serialGC-leak': {\n",
    "        'image' : 'poojakulkarni/javainflaterserialgcleak:latest',\n",
    "        'starttime': '2019-08-07T22:08:53.983Z',\n",
    "        'endtime': '2019-08-08T01:09:58.983Z'\n",
    "    },\n",
    "    'script-javaleak-leak':{\n",
    "        'image' : 'poojakulkarni/javaleakunsafe:latest',\n",
    "        'starttime': '2019-08-06T11:39:54.373Z',\n",
    "        'endtime': '2019-08-06T14:40:54.373Z'\n",
    "    },\n",
    "    'script-java-memory-consuming-object-leak': {\n",
    "        'image' : 'poojakulkarni/javamemoryconsumingobjectleak:latest',\n",
    "        'starttime': '2019-08-07T12:58:53.791Z',\n",
    "        'endtime': '2019-08-07T16:00:08.791Z'\n",
    "    },\n",
    "    'script-java-static-member-leak': {\n",
    "        'image' : 'poojakulkarni/javastaticmemberleak:latest',\n",
    "        'starttime': '2019-08-06T02:30:09.948Z',\n",
    "        'endtime': '2019-08-06T05:31:14.948Z'\n",
    "    },\n",
    "    'script-java-weak-references-leak': {\n",
    "        'image' : 'poojakulkarni/javaweakreferencesleak:latest',\n",
    "        'starttime': '2019-08-05T20:23:36.291Z',\n",
    "        'endtime': '2019-08-05T23:24:46.291Z'\n",
    "    },\n",
    "    'script-js-recursiveclosure-leak': {\n",
    "        'image' : 'poojakulkarni/jsrecursiveclosureleak:latest',\n",
    "        'starttime': '2019-08-12T10:46:05.888Z',\n",
    "        'endtime': '2019-08-12T11:17:10.888Z'\n",
    "    },\n",
    "    'script-js-globalvar-leak' : {\n",
    "        'image' : 'poojakulkarni/jsglobalvariableleak:latest',\n",
    "        'starttime': '2019-08-12T11:19:21.122Z',\n",
    "        'endtime': '2019-08-12T11:50:26.121Z'\n",
    "    },\n",
    "    # 6 LEAKY FIXES\n",
    "    'nodejs-gc-test-leakfix': {\n",
    "        'image' : 'poojakulkarni/nodegctestleakfix:latest',\n",
    "        'starttime': '2019-07-20T09:48:41.477Z',\n",
    "        'endtime': '2019-07-20T12:49:56.477Z'\n",
    "    },\n",
    "    'script-basic-java-sleep1min-leakfix':{ # 1min sleep\n",
    "        'image': 'poojakulkarni/javabasicleak1minsleepfix:latest',\n",
    "        'starttime': '2019-07-19T21:35:03.516Z',\n",
    "        'endtime': '2019-07-20T00:36:18.516Z'\n",
    "    },\n",
    "    'script-mem-alloc-cffi-leakfix':{\n",
    "        'image': 'poojakulkarni/memallocpyleakfix:latest',\n",
    "        'starttime': '2019-07-19T15:28:37.552Z',\n",
    "        'endtime': '2019-07-19T18:29:52.552Z'\n",
    "     },\n",
    "    'noodejs-event-emitter-leakfix':{ # \n",
    "        'image':'poojakulkarni/nodejseventemitterfix:latest',\n",
    "        'starttime': '2019-07-24T12:43:11.554Z',\n",
    "        'endtime': '2019-07-24T15:43:26.554Z'\n",
    "    },\n",
    "    'script-basic-python-sleep1min-verified-leakfix':{\n",
    "        'image': 'poojakulkarni/pythonbasicleak1minsleepfix:latest',\n",
    "        'starttime': '2019-07-20T06:45:33.63Z',\n",
    "        'endtime': '2019-07-20T09:46:43.63Z'\n",
    "    },\n",
    "    'nodejs-basic-fix': {\n",
    "        'image' : 'poojakulkarni/nodejsbasicfix:latest',\n",
    "        'starttime': '2019-07-23T05:13:18.819Z',\n",
    "        'endtime': '2019-07-23T08:13:33.819Z'\n",
    "    },\n",
    "}, \"138.246.232.219\" : {\n",
    "    # ALL THE 6 LEAKY APPLICATIONS\n",
    "    # REGULAR USAGE APPLICATIONS\n",
    "    # these are RUNNING on Different VM - host2\n",
    "    # try to restore, else change host IP and execute\n",
    "    'script-nodejs-buffer-arr-push-app-regular': {\n",
    "        'image' : 'poojakulkarni/nodejsarraypushapply:latest',\n",
    "        'starttime': '2019-07-21T12:53:14.399Z',\n",
    "        'endtime': '2019-07-21T15:54:19.399Z'\n",
    "    },\n",
    "    'script-nodejs-buffer-base64-to-obj-regular': {\n",
    "        'image' : 'poojakulkarni/nodejsbufferbase64toobj:latest',\n",
    "        'starttime': '2019-07-21T15:56:12.128Z',\n",
    "        'endtime': '2019-07-21T18:57:32.128Z'\n",
    "    },\n",
    "    'script-nodejs-buffer-concat-arr-regular': {\n",
    "        'image' : 'poojakulkarni/nodejsbufferconcatwitharrays:latest',\n",
    "        'starttime': '2019-07-20T00:14:32.574Z',\n",
    "        'endtime': '2019-07-20T03:15:47.577Z'\n",
    "    },\n",
    "    'script-nodejs-buffer-concat-buf-regular': {\n",
    "        'image' : 'poojakulkarni/nodejsbufferconcatwithbuffers:latest',\n",
    "        'starttime': '2019-07-20T03:17:41.968Z',\n",
    "        'endtime': '2019-07-20T06:19:01.969Z'\n",
    "    },\n",
    "    'script-nodejs-buffer-offset-insert-regular': {\n",
    "        'image' : 'poojakulkarni/nodejsbufferoffsetinsert:latest',\n",
    "        'starttime': '2019-07-21T00:40:21.958Z',\n",
    "        'endtime': '2019-07-21T03:41:31.958Z'\n",
    "    },\n",
    "    'script-nodejs-buffer-to-obj-regular': {\n",
    "        'image' : 'poojakulkarni/nodejsbuffertoobj:latest',\n",
    "        'starttime': '2019-07-22T19:25:23.526Z',\n",
    "        'endtime': '2019-07-22T22:26:23.526Z'\n",
    "    },\n",
    "    'script-nodejs-sizedbuf-offset-insert-regular': {\n",
    "        'image' : 'poojakulkarni/nodejssizedbufoffsetinsert:latest',\n",
    "        'starttime': '2019-07-20T06:20:52.017Z',\n",
    "        'endtime': '2019-07-20T09:22:17.017Z'\n",
    "    },\n",
    "    'in-memory-analytics-large': {\n",
    "        'image' : 'cloudsuite/in-memory-analytics',\n",
    "        'starttime': '2019-07-25T10:54:04.597Z',\n",
    "        'endtime': '2019-07-25T11:28:54.597Z'\n",
    "    },\n",
    "    'script-java-inflater-concMarkandSweep-leak': {\n",
    "        'image' : 'poojakulkarni/javainflaterconcmarkandsweepleak:latest',\n",
    "        'starttime': '2019-08-12T11:05:39.47Z',\n",
    "        'endtime': '2019-08-12T11:36:34.47Z'\n",
    "    },\n",
    "}  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newCalculateDifferenceBetweenDatapoints(containerSec):\n",
    "    firstRound = True\n",
    "    last5thValue = 0\n",
    "    last5thTimeStamp = 0\n",
    "    every5thItem = 0\n",
    "    results = {}\n",
    "    for item in containerSec:\n",
    "        # For getting every the rate of 30 seconds\n",
    "        if firstRound:\n",
    "            last5thValue = containerSec[item]\n",
    "            last5thTimeStamp = item\n",
    "            firstRound = False\n",
    "        else:\n",
    "            if every5thItem == 2:\n",
    "                # Calculation\n",
    "                # Cut the last 9 numbers away\n",
    "                divider = item - last5thTimeStamp\n",
    "                value = (containerSec[item] - last5thValue)\n",
    "                # Save to the new map\n",
    "                results[item] = (value / divider.total_seconds()) * 100\n",
    "                # Reset with the existing values\n",
    "                last5thValue = containerSec[item]\n",
    "                last5thTimeStamp = item\n",
    "                every5thItem = 0\n",
    "        every5thItem = every5thItem + 1\n",
    "    return results\n",
    "\n",
    "def transformDicToArrays(containerSec):\n",
    "    xresult = []\n",
    "    yresult = []\n",
    "    for item in containerSec:\n",
    "        xresult.append(item)\n",
    "        yresult.append(containerSec.get(item,1)/2)\n",
    "    return xresult, yresult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect(dbname, host):\n",
    "    client = InfluxDBClient(host, port, user, password, dbname)\n",
    "    return client\n",
    "\n",
    "\n",
    "def get_container_memory_usage(client, host):\n",
    "    \"\"\"\n",
    "    prometheus query is as follows -\n",
    "    sum by (name)(container_memory_usage_bytes{image!=“\",container_label_org_label_schema_group=\"\"})\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    \n",
    "    #total machine memory\n",
    "    memTotalQ = \"select value \" \\\n",
    "                \"from node_memory_MemTotal_bytes \" \\\n",
    "                \"where time >= \\'\" + imagedict[host][dbname]['starttime'] +\"\\' \" + \\\n",
    "                \"AND time <= \\'\" + imagedict[host][dbname]['endtime'] + \"\\'\"\n",
    "\n",
    "    memTotal_rs = client.query(memTotalQ)\n",
    "    memTotal = get_dataframe(memTotal_rs, \"total\")\n",
    "    \n",
    "\n",
    "    #Container memory usage\n",
    "\n",
    "    containerMemUsageQ = \"SELECT value as usage \" \\\n",
    "                         \"FROM container_memory_usage_bytes \" \\\n",
    "                         \"WHERE image=\" + \"\\'\" + imagedict[host][dbname]['image'] + \"\\' \" + \\\n",
    "                         \"AND container_label_org_label_schema_group='' \" \\\n",
    "                         \"AND time >= \\'\" + imagedict[host][dbname]['starttime'] +\"\\' \" +\\\n",
    "                         \"AND time <= \\'\" + imagedict[host][dbname]['endtime'] + \"\\'\"\n",
    "\n",
    "    #print(containerMemUsageQ)\n",
    "    containerMemUsage_rs = client.query(containerMemUsageQ)\n",
    "    containerMemUsage = get_dataframe(containerMemUsage_rs, \"usage\")\n",
    "    \n",
    "    containerMemUsage['usage'] = (containerMemUsage['usage']/memTotal['total'].values[0])*100\n",
    "    containerMemUsage['time']  = pd.to_datetime(containerMemUsage['time'])\n",
    "    #print(containerMemUsage)\n",
    "\n",
    "\n",
    "    return containerMemUsage\n",
    "\n",
    "\n",
    "def get_host_memory_usage(client, host):\n",
    "    \"\"\"\n",
    "    prometheus query:\n",
    "    node_memory_MemTotal_bytes - (node_memory_MemFree_bytes+node_memory_Buffers_bytes+node_memory_Cached_bytes)\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    #total memory\n",
    "    memTotalQ = \"select value \" \\\n",
    "                \"from node_memory_MemTotal_bytes \" \\\n",
    "                \"where time >= \\'\" + imagedict[host][dbname]['starttime'] +\"\\' \" + \\\n",
    "                \"AND time <= \\'\" + imagedict[host][dbname]['endtime'] + \"\\'\"\n",
    "\n",
    "    memTotal_rs = client.query(memTotalQ)\n",
    "    memTotal = get_dataframe(memTotal_rs, \"total\")\n",
    "\n",
    "    #free memory\n",
    "    memFreeQ = \"select value \" \\\n",
    "               \"from node_memory_MemFree_bytes \" \\\n",
    "               \"where time >= \\'\" + imagedict[host][dbname]['starttime'] +\"\\' \" + \\\n",
    "               \"AND time <= \\'\" + imagedict[host][dbname]['endtime'] + \"\\'\"\n",
    "\n",
    "    memFree_rs = client.query(memFreeQ)\n",
    "    memFree = get_dataframe(memFree_rs, \"free\")\n",
    "\n",
    "    #buffered\n",
    "    memBufferQ = \"select value \" \\\n",
    "                 \"from node_memory_Buffers_bytes \" \\\n",
    "                 \"where time >= \\'\" + imagedict[host][dbname]['starttime'] + \"\\' \" + \\\n",
    "                 \"AND time <= \\'\" + imagedict[host][dbname]['endtime'] + \"\\'\"\n",
    "    memBuffer_rs = client.query(memBufferQ)\n",
    "    memBuffer = get_dataframe(memBuffer_rs, \"buffer\")\n",
    "\n",
    "    #cached\n",
    "    memCachedQ = \"select value \" \\\n",
    "                 \"from node_memory_Cached_bytes \" \\\n",
    "                 \"where time >= \\'\" + imagedict[host][dbname]['starttime'] + \"\\' \" + \\\n",
    "                 \"AND time <= \\'\" + imagedict[host][dbname]['endtime'] + \"\\'\"\n",
    "\n",
    "    memCached_rs = client.query(memCachedQ)\n",
    "    memCache = get_dataframe(memCached_rs, \"cache\")\n",
    "\n",
    "    #join dataframes for easier plotting\n",
    "    tf_merge = pd.merge(memTotal, memFree, on=\"time\")\n",
    "    tfc_merge = pd.merge(tf_merge, memCache, on=\"time\")\n",
    "    tfcb_merge = pd.merge(tfc_merge, memBuffer, on=\"time\")\n",
    "\n",
    "    #add a column based on operation (used = total - {free+cache+buffer})\n",
    "    tfcb_merge.apply(lambda row: row.total - (row.free + row.buffer + row.cache), axis=1)\n",
    "    tfcb_merge['used'] = tfcb_merge.apply(\n",
    "        lambda row: row.total - (row.free + row.buffer + row.cache),\n",
    "        axis=1)\n",
    "    tfcb_merge['time']  = pd.to_datetime(tfcb_merge['time'])\n",
    "    tfcb_merge['used'] = (tfcb_merge['used']/memTotal['total'].values[0])*100\n",
    "    # return final dataframe to plot\n",
    "\n",
    "    return tfcb_merge\n",
    "\n",
    "\n",
    "def get_container_cpu_usage(client, host):\n",
    "    \"\"\"\n",
    "    sum by (name) (rate(container_cpu_usage_seconds_total{image!=\"\",container_label_org_label_schema_group=\"\"}[1m])) / scalar(count(node_cpu_seconds_total{mode=\"user\"})) * 100\n",
    "\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    \n",
    "    q = \"select value \" \\\n",
    "        \"from container_cpu_user_seconds_total \" \\\n",
    "        \"where image=\" + \"\\'\" + imagedict[host][dbname]['image'] \\\n",
    "        + \"\\'\"\\\n",
    "        \"AND time >= \\'\" + imagedict[host][dbname]['starttime'] + \"\\'\"  \\\n",
    "        \"AND time <= \\'\" + imagedict[host][dbname]['endtime'] + \"\\'\" \n",
    "    \n",
    "    qrs = client.query(q)\n",
    "\n",
    "    containerCPUUserSecondsTotal = {}\n",
    "    \n",
    "    containerResult = qrs\n",
    "    containerPoints = containerResult.get_points()\n",
    "\n",
    "    for item in containerPoints:\n",
    "        datetime_object = datetime.strptime(item['time'], '%Y-%m-%dT%H:%M:%S.%fZ')\n",
    "        containerCPUUserSecondsTotal[datetime_object] = item['value']\n",
    "        \n",
    "    results = newCalculateDifferenceBetweenDatapoints(containerCPUUserSecondsTotal)\n",
    "    xresult, yresult = transformDicToArrays(results)\n",
    "    df = pd.DataFrame({'time':xresult, 'percentage':yresult})\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    df['time']  = pd.to_datetime(df['time'])\n",
    "    df_cut_xtreme = df\n",
    "    return df_cut_xtreme\n",
    "\n",
    "\n",
    "def get_host_cpu_usage(client, host):\n",
    "    \"\"\"\n",
    "    sum(rate(container_cpu_user_seconds_total{image!=\"\"}[1m])) / count(node_cpu_seconds_total{mode=\"user\"}) * 100\n",
    "sum(rate(node_cpu_seconds_total[1m])) by (mode) * 100 / scalar(count(node_cpu_seconds_total{mode=\"user\"}))\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    q = \"SELECT value \" \\\n",
    "        \"FROM node_cpu_seconds_total \" \\\n",
    "        \"WHERE time >= \\'\" + imagedict[host][dbname]['starttime'] + \"\\'\"  \\\n",
    "        \"AND time <= \\'\" + imagedict[host][dbname]['endtime'] + \"\\'\" \\\n",
    "        \"AND mode='idle' \"\n",
    "    qrs = client.query(q)\n",
    "    \n",
    "        \n",
    "    containerCPUUserSecondsTotal = {}\n",
    "    \n",
    "    containerResult = qrs\n",
    "    containerPoints = containerResult.get_points()\n",
    "\n",
    "\n",
    "    for item in containerPoints:\n",
    "        datetime_object = datetime.strptime(item['time'], '%Y-%m-%dT%H:%M:%S.%fZ')\n",
    "        containerCPUUserSecondsTotal[datetime_object] = item['value']\n",
    "    \n",
    " \n",
    "    results = newCalculateDifferenceBetweenDatapoints(containerCPUUserSecondsTotal)\n",
    "    xresult, yresult = transformDicToArrays(results)\n",
    "    df = pd.DataFrame({'time':xresult, 'percentage':yresult})\n",
    "\n",
    "    df['percentage'] = 100 - df['percentage'] \n",
    "    \n",
    "    \n",
    "    df['time']  = pd.to_datetime(df['time'])\n",
    "    df_cut_xtreme = df\n",
    "    return df_cut_xtreme\n",
    "\n",
    "def get_dataframe(query_resultset, valuename):\n",
    "    qrs_json = query_resultset.raw\n",
    "    np_list = qrs_json.get('series')[0].get('values')\n",
    "    df = pd.DataFrame(np_list, columns=[\"time\", valuename])\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_df(df, name, col_to_plot, ymax, ylabelle):\n",
    "    \"\"\"\n",
    "    Plots the graph and saves the graph image in working dir\n",
    "    :param df: dataframe to plot\n",
    "    :param col_to_plot: specifies which metrics to plot\n",
    "    :param name: name of the feature (used to save the plot locally)\n",
    "    :param ymax: to limit percentage plots to 100 on y axis\n",
    "    :param ylabelle : y label\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    colors = ['blue', 'orange', 'red', 'grey', 'green', 'black', 'brown']\n",
    "    ax = plt.gca()\n",
    "    count = 0\n",
    "    for col in col_to_plot:\n",
    "        df.plot(kind='line', x=\"time\", y=col, color=colors[count], ax=ax)\n",
    "        count = count+1\n",
    "\n",
    "    plt.title(name)\n",
    "    plt.ylabel(ylabelle)\n",
    "    #ax.tick_params(axis='x', rotation=16)\n",
    "    plt.grid(True)\n",
    "    #plt.savefig(\"plots/\"+dbname+\"/\"+name+\".png\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.clf()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_linear_regression(X, y , window=50):    \n",
    "    assert len(X)==len(y)\n",
    "    \n",
    "    regressor = linear_model.LinearRegression()\n",
    "\n",
    "    win = 1\n",
    "    for iStart in range(0, len(X)-window): \n",
    "        # last index of the window\n",
    "        alarm = 0\n",
    "        iEnd = iStart+window\n",
    "        \n",
    "        # train model\n",
    "        regressor.fit(X[iStart:iEnd], y[iStart:iEnd])\n",
    "        \n",
    "        # note slope and intercept\n",
    "        b = regressor.intercept_\n",
    "        m = regressor.coef_\n",
    "        \n",
    "        # find the last value in the window is maximum - this could indicate the memory is continuously growing \n",
    "        X_window = X[iStart:iEnd]\n",
    "        y_window = y[iStart:iEnd]\n",
    "        y_window_max = y[iStart:iEnd].max()\n",
    "        \n",
    "        if (y_window_max == y_window[-1]):\n",
    "            #print(\"indication of probable continuous memory growth\")\n",
    "            alarm = alarm + 1\n",
    "            growing = True\n",
    "            # window size is 50(a random number for now), \n",
    "            # 50 window size considered is 50,\n",
    "            if alarm > 50 :\n",
    "                print(\"window kind of tumbled but memory still is growing ...\")\n",
    "                # this could be a point to alert user about critical time, low priority?\n",
    "                # exception when application starts\n",
    "                \n",
    "        else :\n",
    "            # memory is being reclaimed : reset alarm\n",
    "            # print(\"memory resource reclaimed, resetting \")\n",
    "            alarm = 0\n",
    "            growing = False\n",
    "            \n",
    "        # or if max is already more than 50% consumed and pattern is growing, calculate critical time\n",
    "        if y_window_max > 50 and growing :\n",
    "            current_last_entry_unixtime = X_window[-1]\n",
    "            current_last_entry_datetime = pd.to_datetime(current_last_entry_unixtime[0], unit='ns')\n",
    "    \n",
    "            time_delta = calculate_critical_time(m, b, current_last_entry_datetime)\n",
    "        \n",
    "            alert_user(time_delta)\n",
    "        \n",
    "    y_pred = regressor.predict(X)\n",
    "    rmse = sqrt(mean_squared_error(y, y_pred))\n",
    "    r2 = r2_score(y, y_pred)\n",
    "    \n",
    "    last_timestamp = pd.to_datetime(X[-1][0], unit='ns')\n",
    "    time_delta = calculate_critical_time(m, b, last_timestamp)\n",
    "    \n",
    "    return y_pred, r2, rmse, time_delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alert_user(time_delta):\n",
    "    \"\"\"\n",
    "    This function notifies if the host memory usage is growing fast,\n",
    "    and if time to reach critical usage 100% (correct??!) is less\n",
    "    than 48hours, then alerts the user about the usage\n",
    "    \"\"\"\n",
    "    \n",
    "    days, hours, minutes = time_delta.days, time_delta.seconds // 3600, time_delta.seconds // 60 % 60\n",
    "    \n",
    "    if (days * 24 + hours) < 5 * 24 :\n",
    "        return \"Leak-Red alert\"\n",
    "    \n",
    "    if (days * 24 + hours) < 10 * 24 :\n",
    "        return \"Leak-Yellow alert\"\n",
    "    \n",
    "    if (days * 24 + hours) < 15 * 24 : # ~ 6days\n",
    "        return \"Leak-Maybee\"\n",
    "    \n",
    "    else:\n",
    "        return \"Not a Leak\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_point_detection(df, feature):\n",
    "    # Currently we no dataset for verification\n",
    "    X = pd.to_numeric(df.time).values\n",
    "    Y = df[feature].values\n",
    "    y = np.asarray(Y)\n",
    "    change_points_df = pd.DataFrame({\"time\":[], feature:[]})\n",
    "    high_pts_df = pd.DataFrame({\"time\":[], feature:[]})\n",
    "    low_pts_df = pd.DataFrame({\"time\":[], feature:[]})\n",
    "    \n",
    "    raising = False\n",
    "    falling = False\n",
    "    \n",
    "    for i in range(1, len(y)-1):\n",
    "        diff = y[i] - y[i-1]\n",
    "        if falling or raising:\n",
    "            \"\"\"\n",
    "            either of the edges are already names and the\n",
    "            current difference must match the result\n",
    "            \"\"\"\n",
    "            if falling :\n",
    "                #print(\"Falling edge\")\n",
    "                # check is difference is negative\n",
    "                if diff < 0:\n",
    "                    continue\n",
    "                else:\n",
    "                    # change point detected\n",
    "                    # print(\"change detected, record point and reset the trails at :\"+ str(i-1))\n",
    "                    # print(X[i])\n",
    "                    # print(y[i])\n",
    "                    temp = pd.DataFrame({\"time\" : [X[i-1],], feature : [y[i-1],]})\n",
    "                    change_points_df = change_points_df.append(temp,\n",
    "                                                               ignore_index=True)\n",
    "                    # in falling edge if raise is detected it is a low point\n",
    "                    low_pts_df = low_pts_df.append(temp, ignore_index=True)\n",
    "                    \n",
    "                    # reset flags\n",
    "                    raising = False\n",
    "                    falling = False\n",
    "            else :\n",
    "                #print(\"Raising edge\")\n",
    "                if diff >= 0:\n",
    "                    continue\n",
    "                else:\n",
    "                    # change point detected\n",
    "                    # print(\"change detected,record point and reset the trails at : \" + str(i-1))\n",
    "                    # print(X[i])\n",
    "                    # print(y[i])\n",
    "                    temp = pd.DataFrame({\"time\" : [X[i-1],], feature : [y[i-1],]})\n",
    "                    \n",
    "                    change_points_df = change_points_df.append(temp,\n",
    "                                                               ignore_index=True)\n",
    "                    # change point ona . raising edge indicates high point \n",
    "                    high_pts_df = high_pts_df.append(temp, ignore_index=True)\n",
    "                    # reset flags\n",
    "                    raising = False\n",
    "                    falling = False\n",
    "                    \n",
    "        elif diff < 0:\n",
    "            # print(\"Fall point detected\")\n",
    "            # indicates a probable falling trend\n",
    "            falling = True\n",
    "            \n",
    "            temp = pd.DataFrame({\"time\" : [X[i-1],], feature : [y[i-1],]})\n",
    "            change_points_df = change_points_df.append(temp,\n",
    "                                                       ignore_index=True)\n",
    "            #low_pts_df = low_pts_df.append(temp, ignore_index=True)\n",
    "            continue\n",
    "        else:\n",
    "            # print(\"Raise point detected\")\n",
    "            # indicates same val or raising trend\n",
    "            raising = True\n",
    "            temp = pd.DataFrame({\"time\" : [X[i-1],], feature : [y[i-1],]})\n",
    "            change_points_df = change_points_df.append(temp,\n",
    "                                                       ignore_index=True)\n",
    "            \n",
    "            #high_pts_df = high_pts_df.append(temp, ignore_index=True)\n",
    "            continue\n",
    "            \n",
    "    return high_pts_df, low_pts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_critical_time(m, b, current_timestamp):\n",
    "    \"\"\"\n",
    "    m - slope\n",
    "    b - intercept of a fitted line\n",
    "    \"\"\"\n",
    "    #print(\"M  : \" + str(m))\n",
    "    # time to reach y = 100\n",
    "    critical_unixtime = (100 - b) / m\n",
    "    #print(critical_unixtime)\n",
    "    critical_pd_datetime = pd.to_datetime(critical_unixtime[0][0], unit='ns')\n",
    "    \n",
    "    # time remaining to reach critical usage = critical_time - current_time\n",
    "    time_delta = critical_pd_datetime - current_timestamp\n",
    "    \n",
    "    return time_delta\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_theilsen(X, y):\n",
    "    res = stats.theilslopes(y, X, 0.95)\n",
    "    # print(res)\n",
    "    # print('Thiel-Sen linear model coefficients, intercept = ' + str(res[1]) + '. Slope = ' + str(res[0]) +'\\n')\n",
    "    \n",
    "    m = res[0]\n",
    "    b = res[1]\n",
    "    \n",
    "    critical_unixtime = (100 - b) / m\n",
    "    critical_unixtime_arr = np.asarray(critical_unixtime, dtype=np.float64)\n",
    "    critical_pd_datetime = pd.to_datetime(critical_unixtime_arr, unit='ns')\n",
    "    \n",
    "    y_pred_list = []\n",
    "    for x in X:\n",
    "        y_pred_list.append(res[1] + res[0] * x)\n",
    "    \n",
    "    y_pred = np.array(y_pred_list)\n",
    "    r2 = r2_score(y, y_pred)\n",
    "    rmse = sqrt(mean_squared_error(y, y_pred))\n",
    "    \n",
    "    last_timestamp = pd.to_datetime(X[-1][0], unit='ns')\n",
    "    #print(last_timestamp)\n",
    "    time_delta = critical_pd_datetime - last_timestamp\n",
    "    \n",
    "    return y_pred, r2, rmse, time_delta\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_linear_regression(X, y):\n",
    "    # Train : simple linear regresson\n",
    "    \n",
    "    regressor = LinearRegression()  \n",
    "    regressor.fit(X, y) \n",
    "    \n",
    "    # To retrieve the intercept:\n",
    "    b = regressor.intercept_\n",
    "    m = regressor.coef_\n",
    "\n",
    "    print(m)\n",
    "    y_pred = regressor.predict(X)\n",
    "    \n",
    "    # calculate time to reach 100% usage : critical time\n",
    "    critical_unixtime = (100 - b) / m\n",
    "    print(critical_unixtime)\n",
    "    criticl_pd_datetime = pd.to_datetime(critical_unixtime[0][0], unit='ns')\n",
    "    \n",
    "    # calculate time required in hours to reach critical time\n",
    "    #current_last_entry_unixtime = X[-1]\n",
    "    #current_last_entry_datetime = pd.to_datetime(current_last_entry_unixtime[0], unit='ns')\n",
    "    # print(pd.to_datetime(df['time'].iloc[-1], unit='ns'))\n",
    "    #y_at_current_last_entry_time = m * current_last_entry_unixtime + b\n",
    "    \n",
    "    #print(\"last_point_of_fit : \" + str(current_last_entry_datetime) + \", \"+ str(y_at_current_last_entry_time))\n",
    "    \n",
    "    last_timestamp = pd.to_datetime(X[-1][0], unit='ns')\n",
    "    time_delta = calculate_critical_time(m, b, last_timestamp)\n",
    "    \n",
    "    rmse = sqrt(mean_squared_error(y, y_pred))\n",
    "    \n",
    "    r2 = r2_score(y, y_pred)\n",
    "    \n",
    "    return y_pred, r2, rmse, time_delta\n",
    "    \n",
    "    #txt = \"critical time : \"+ str(time_delta)+\" \\n RMSE :\" + str(rmse)+\"\"\n",
    "    #plt.figtext(0.5, 0.01, txt, wrap=True, horizontalalignment='center', fontsize=15)\n",
    "    #plt.savefig(\"./plots/\"+dbname+\"/simple_LR.png\")\n",
    "    #plt.show()\n",
    "    #plt.clf()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_fit(df, feature, applicationName, results):\n",
    "    # extract X and y\n",
    "    \n",
    "    df = df.set_index(\"time\")\n",
    "    df = df.resample(\"2s\").median().interpolate()\n",
    "    \n",
    "    df = df.rolling('1s').mean()\n",
    "    #print(df)\n",
    "    df = df.replace([np.inf, -np.inf], np.nan)\n",
    "    df = df.dropna()\n",
    "    \n",
    "    df = df.reset_index()\n",
    "    \n",
    "    X = pd.to_numeric(df.time).values.reshape(-1, 1)\n",
    "    y = df[feature].values.reshape(-1, 1)\n",
    "    \n",
    "    assert len(X)==len(y)\n",
    "    \n",
    "    # scatter all data points\n",
    "    plt.scatter(X, y,  color='yellow')\n",
    "    \n",
    "    # Linear Regression\n",
    "    y_lr_pred, slr_r2, slr_rmse, slr_tc = fit_linear_regression(X, y)\n",
    "    prediction = alert_user(slr_tc) # fill in the function which predicts based on tc\n",
    "    \"\"\"\n",
    "    results.add_row([applicationName[-30:],\n",
    "                     \"LR\",\n",
    "                     \"%.6f\" % slr_r2,\n",
    "                     \"%.6f\" % slr_rmse,\n",
    "                     slr_tc,\n",
    "                     prediction])\n",
    "    \"\"\"\n",
    "    # Theilsen slope estimator\n",
    "    y_ts_pred, ts_r2, ts_rmse, ts_tc = fit_theilsen(X, y)\n",
    "    prediction = alert_user(ts_tc) # fill in the function which predicts based on tc\n",
    "    \n",
    "    results.add_row([applicationName[-30:],\n",
    "                     \"TS\",\n",
    "                     \"%.6f\" % ts_r2,\n",
    "                     \"%.6f\" % ts_rmse,\n",
    "                     ts_tc,\n",
    "                     prediction])\n",
    "    \n",
    "                     \n",
    "    # change point detection to find change points in the trend \n",
    "    high_pts_df, low_pts_df = change_point_detection(df, feature)\n",
    "    \n",
    "    X_highpts = pd.to_numeric(high_pts_df.time).values.reshape(-1, 1)\n",
    "    y_highpts = high_pts_df[feature].values.reshape(-1, 1)\n",
    "    \n",
    "    # Apply Linear regression to high points \n",
    "    # Linear Regression \n",
    "    y_hp_lr_pred, hp_slr_r2, hp_slr_rmse, hp_slr_tc = fit_linear_regression(X_highpts, y_highpts)\n",
    "    prediction = alert_user(hp_slr_tc) # fill in the function which predicts based on tc\n",
    "    \n",
    "    results.add_row([applicationName[-30:],\n",
    "                     \"CPD:LR\",\n",
    "                     \"%.6f\" % hp_slr_r2,\n",
    "                     \"%.6f\" % hp_slr_rmse,\n",
    "                     hp_slr_tc,\n",
    "                     prediction])\n",
    "    \n",
    "    # Theil-sen estimator\n",
    "    y_hp_ts_pred, hp_ts_r2, hp_ts_rmse, hp_ts_tc = fit_theilsen(X_highpts, y_highpts)\n",
    "    prediction = alert_user(hp_ts_tc) # fill in the function which predicts based on tc\n",
    "    \n",
    "    results.add_row([applicationName[-30:],\n",
    "                     \"CPD:TS\",\n",
    "                     \"%.6f\" % hp_ts_r2,\n",
    "                     \"%.6f\" % hp_ts_rmse,\n",
    "                     hp_ts_tc,\n",
    "                     prediction])\n",
    "    \n",
    "    \"\"\"\n",
    "    X_lowpts = ...\n",
    "    y_lowpts = ...\n",
    "    \n",
    "    plt.scatter(X_lowpts, y_lowpts,  color='blue', marker='x')\n",
    "    \n",
    "    # Linear Regression \n",
    "    y_lp_lr_pred, lp_slr_r2, lp_slr_rmse, lp_slr_tc = fit_linear_regression(X_lowpts, y_lowpts)\n",
    "    \n",
    "    # Theil-sen estimator\n",
    "    y_lp_ts_pred, lp_ts_r2, lp_ts_rmse, lp_ts_tc = fit_theilsen(X_lowpts, y_lowpts)\n",
    "    \n",
    "    y_rw_pred, rw_r2, rw_rmse, rw_tc = rolling_linear_regression(X, y)\n",
    "    \"\"\"\n",
    "    \n",
    "    # plot all estimators\n",
    "    assert len(X)==len(y_lr_pred)\n",
    "    \n",
    "    #plt.plot(X, y_lr_pred, color='red', linewidth=2, label = \"Simple Linear Regression\")\n",
    "    plt.plot(X, y_ts_pred, color='green', linewidth=2, label = \"Theilsen estimator\")\n",
    "    \n",
    "    #plt.scatter(X_highpts, y_highpts, color = 'indigo', marker='x', s = 40)\n",
    "    \n",
    "    #plt.plot(X_highpts, y_hp_lr_pred, color='turquoise', linewidth=2, label = \"high change point - Linear Regression\")\n",
    "    #plt.plot(X_highpts, y_hp_ts_pred, color='orange', linewidth=2, label = \"high change point - Theilsen estimator\")\n",
    "    \n",
    "    plt.legend(loc='upper left')\n",
    "    plt.title(str(applicationName))\n",
    "    \n",
    "    # add a row in the results table\n",
    "    print(results)\n",
    "    plt.show()\n",
    "    \n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # connect to the db\n",
    "    # todo dbname must be parameterised to be passed as an argument\n",
    "    t = PrettyTable([\"AppName\", \"Algorithm\" , \"R2\", \"RMSE\", \"Critical Time\", \"Classification-Prediction\"])\n",
    "    \n",
    "    for host in imagedict.keys():\n",
    "        print(host)\n",
    "        for item in imagedict[host].keys():\n",
    "            print(\"RUNNING : \"+ item)\n",
    "            dbname = item\n",
    "            client = connect(item, host)\n",
    "\n",
    "            host_mem_used_df = get_host_memory_usage(client, host)\n",
    "            plot_df(host_mem_used_df, \"host_mem\", ['used'], 0, \"Host Memory Usage Percentage\"+item)\n",
    "\n",
    "            container_mem_usage_df = get_container_memory_usage(client, host)\n",
    "            #plot_df(container_mem_usage_df, \"container_mem\", ['usage'], 0, \"Container Memory Usage\"+item)\n",
    "\n",
    "            host_cpu_usage = get_host_cpu_usage(client, host)\n",
    "            #plot_df(host_cpu_usage, \"host_cpu\", ['percentage'], 100, \"Host CPU Usage\"+item)\n",
    "\n",
    "            container_cpu_usage = get_container_cpu_usage(client, host)\n",
    "            #plot_df(container_cpu_usage, \"container_cpu\", ['percentage'], 0, \"Container CPU Usage\"+item)\n",
    "\n",
    "            used_df = host_mem_used_df[['time', 'used']].copy()\n",
    "\n",
    "            try_fit(used_df, \"used\" , item, t)\n",
    "            \n",
    "    \"\"\"\n",
    "    #For only selected dbs, if needed\n",
    "    dbname = item = \"nodejs-basic-leak\"\n",
    "    host = \"138.246.232.174\"\n",
    "    \n",
    "    client = connect(item, host)\n",
    "\n",
    "    host_mem_used_df = get_host_memory_usage(client, host)\n",
    "    plot_df(host_mem_used_df, \"host_mem\", ['used'], 0, \"Host Memory Usage Percentage\"+item)\n",
    "\n",
    "    container_mem_usage_df = get_container_memory_usage(client, host)\n",
    "    #plot_df(container_mem_usage_df, \"container_mem\", ['usage'], 0, \"Container Memory Usage\"+item)\n",
    "\n",
    "    host_cpu_usage = get_host_cpu_usage(client, host)\n",
    "    #plot_df(host_cpu_usage, \"host_cpu\", ['percentage'], 100, \"Host CPU Usage\"+item)\n",
    "    \n",
    "    container_cpu_usage = get_container_cpu_usage(client, host)\n",
    "    #plot_df(container_cpu_usage, \"container_cpu\", ['percentage'], 0, \"Container CPU Usage\"+item)\n",
    "\n",
    "    used_df = host_mem_used_df[['time', 'used']].copy()\n",
    "\n",
    "    try_fit(used_df, \"used\" , item, t)\n",
    "    \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
